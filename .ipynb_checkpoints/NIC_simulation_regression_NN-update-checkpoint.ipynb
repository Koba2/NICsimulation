{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import *\n",
    "from scipy.optimize import minimize\n",
    "from copy import deepcopy\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.optimizer_hooks import WeightDecay\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in = 13\n",
    "num_h = 64\n",
    "num_out = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "x,y = boston.data,boston.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "x,y = diabetes.data,diabetes.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#これは正規化が必要な時に行う、一応やったほうが安全？\n",
    "mn = MinMaxScaler(feature_range=(-1,1),copy = True)\n",
    "x_train = mn.fit_transform(x_train)\n",
    "x_test = mn.fit_transform(x_test)\n",
    "\n",
    "x_train[x_train < -1] = -1\n",
    "x_train[x_train > 1] = 1\n",
    "x_test[x_test < -1] = -1\n",
    "x_test[x_test > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(n_in, n_hidden)\n",
    "            self.l2 = L.Linear(n_hidden, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.l1(x))\n",
    "        h = self.l2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(chainer.Chain):\n",
    "\n",
    "    def __init__(self,n_in=1,n_hidden=3,n_out=1):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(n_in,n_hidden)\n",
    "            self.l2 = L.Linear(n_hidden,n_hidden)\n",
    "            self.l3 = L.Linear(n_hidden,n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.l1(x))\n",
    "        h = F.relu(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x18a17827788>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net1(n_in=num_in, n_hidden=num_h, n_out=num_out)\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "for epoch in range(n_epoch):\n",
    "    # データセット並べ替えた順番を取得\n",
    "    order = np.random.permutation(range(len(x_train)))\n",
    "    for i in range(len(order)):\n",
    "        # バッチを準備\n",
    "        index = order[i]\n",
    "        x_train_batch = x_train[index:index+1]\n",
    "        y_train_batch = y_train[index:index+1].reshape(1,1)\n",
    "\n",
    "        # 予測値を出力\n",
    "        y_pred = net(x_train_batch)\n",
    "        # 目的関数を適用\n",
    "        loss_train_batch = F.squared_error(y_train_batch, y_pred)*0.5\n",
    "\n",
    "        # 勾配のリセットと勾配の計算\n",
    "        net.cleargrads()\n",
    "        loss_train_batch.backward()\n",
    "\n",
    "        # パラメータの更新\n",
    "        optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ここまでがN.N.の学習フェーズ\n",
    "#ここからNICの計算フェーズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = np.array([0]*(num_h*(num_in+num_out+1) + num_out),dtype=np.float32) #ここで学習済パラメータを1次元化する\n",
    "for i in range(num_in*num_h):\n",
    "    par[i] = net.l1.W.array[i//num_in][i%num_in]\n",
    "for i in range(num_in*num_h,(num_in+1)*num_h):\n",
    "    par[i] = net.l1.b.array[i%num_h]\n",
    "for i in range((num_in+1)*num_h,(num_in+num_out+1)*num_h):\n",
    "    par[i] = net.l2.W.array[(i-(num_in+1)*num_h)//num_h][(i-(num_in+1)*num_h)%num_h]\n",
    "for i in range((num_in+num_out+1)*num_h ,num_h*(num_in+num_out+1)+num_out):\n",
    "    par[i] = net.l2.b.array[(i-(num_in+num_out+1)*num_h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_after_optimization(chainer.Chain):\n",
    "    def __init__(self,W_l1,b_l1,W_l2,b_l2,n_in,n_hidden,n_out):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(n_in, n_hidden,initialW=W_l1,initial_bias=b_l1)\n",
    "            self.l2 = L.Linear(n_hidden, n_out,initialW=W_l2,initial_bias=b_l2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.l1(x))\n",
    "        h = self.l2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(x,y,theta):\n",
    "    #x:入力値、y:出力値、theta:学習後のパラメータベクトル,x,yは1次元のndarray(shape=(1,)となっている)\n",
    "    #まず学習済パラメータを用いてモデルを構成し、その後損失関数を計算する\n",
    "    #もし使うNNの形を変えるならこの構成部分をのコードを変える必要がある\n",
    "\n",
    "    W1 = np.array([[0 for _ in range(num_in)] for _ in range(num_h)],dtype = np.float32)\n",
    "    b1 = np.array([0 for _ in range(num_h)],dtype = np.float32)\n",
    "    W2 = np.array([[0 for _ in range(num_h)] for _ in range(num_out)],dtype = np.float32)\n",
    "    b2 = np.array([0 for _ in range(num_out)],dtype = np.float32)\n",
    "\n",
    "    for i in range(num_in*num_h):\n",
    "        W1[i//num_in][i%num_in] = theta[i]\n",
    "    for i in range(num_in*num_h,(num_in+1)*num_h):\n",
    "        b1[i%num_h] = theta[i]\n",
    "    for i in range((num_in+1)*num_h, (num_in+num_out+1)*num_h):\n",
    "        W2[(i-(num_in+1)*num_h)//num_h][(i-(num_in+1)*num_h)%num_h] = theta[i]\n",
    "    for i in range((num_in+num_out+1)*num_h, (num_in+num_out+1)*num_h+num_out):\n",
    "        b2[i-(num_in+num_out+1)*num_h] = theta[i]\n",
    "\n",
    "    new_net = Net_after_optimization(n_in = num_in, n_hidden=num_h, n_out = num_out, W_l1=W1,b_l1=b1,W_l2=W2,b_l2=b2)\n",
    "    \n",
    "    y_pred = new_net(x.reshape(1,len(x_train[0]))).array[0][0]\n",
    "    return 0.5*((y-y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient1(loss,x,y,theta,num):\n",
    "    #loss: 使うloss関数、x,y:i番目のデータ,theta:今のパラメータベクトル,num: 何番目のパラメータについて偏微分をとるか\n",
    "    #出力は勾配ベクトル\n",
    "    h = 1e-4\n",
    "    theta1,theta2 = theta.copy(),theta.copy()\n",
    "    theta1[num] += h\n",
    "    theta2[num] -= h\n",
    "    return (loss(x,y,theta1)-loss(x,y,theta2))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient2(loss,x,y,theta,num1,num2):\n",
    "    h = 1e-4\n",
    "    if num1 == num2:\n",
    "        theta1,theta2 = theta.copy(),theta.copy()\n",
    "        theta1[num1] += h\n",
    "        theta2[num1] -= h\n",
    "        return (loss(x,y,theta1) + loss(x,y,theta2) -2*loss(x,y,theta))/(h**2)\n",
    "    else:\n",
    "        theta1,theta2,theta3,theta4 = theta.copy(),theta.copy(),theta.copy(),theta.copy()\n",
    "        theta1[num1],theta1[num2] = theta[num1]+h,theta[num2]+h\n",
    "        theta2[num1],theta2[num2] = theta[num1]+h,theta[num2]-h\n",
    "        theta3[num1],theta3[num2] = theta[num1]-h,theta[num2]+h\n",
    "        theta4[num1],theta4[num2] = theta[num1]-h,theta[num2]-h\n",
    "        return (loss(x,y,theta1) + loss(x,y,theta4) - loss(x,y,theta2) - loss(x,y,theta3))/(4*(h**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_ast(par,loss):\n",
    "    #入力:学習後のパラメータ、出力:行列G^ast\n",
    "    n = len(par)\n",
    "    G = np.array([[0 for _ in range(n)] for _ in range(n)])\n",
    "    E_x = [0]*n    #E_x[i] = i番目の偏微分の期待値\n",
    "    E_xy = [[0 for _ in range(n)] for _ in range(n)]  #E_xy[i][j] = i,j番目の偏微分の期待値\n",
    "    for i in range(n):\n",
    "        element = 0\n",
    "        for x,y in zip(x_train,y_train):\n",
    "            element += compute_gradient1(loss,x,y,par,i)\n",
    "        element /= float(len(x_train))\n",
    "        E_x[i] = element\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i <= j:\n",
    "                element = 0\n",
    "                for x,y in zip(x_train,y_train):\n",
    "                    element += compute_gradient1(loss,x,y,par,i)*compute_gradient1(loss,x,y,par,j)\n",
    "                element /= float(len(x_train))\n",
    "                E_xy[i][j] = element\n",
    "            else:\n",
    "                E_xy[i][j] = E_xy[j][i]\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            G[i][j] = E_xy[i][j] - E_x[i]*E_x[j]\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_ast(par,loss):\n",
    "    #入力:学習後のパラメータ、出力:行列Q^ast\n",
    "    n = len(par)\n",
    "    Q = np.array([[0 for _ in range(n)] for _ in range(n)])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            element = 0\n",
    "            for x,y in zip(x_train,y_train):\n",
    "                element += compute_gradient2(loss,x,y,par,i,j)\n",
    "            element /= float(len(x_train))\n",
    "            Q[i][j] = element\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_ast(par,loss):\n",
    "    res = 0\n",
    "    for x,y in zip(x_train,y_train):\n",
    "        res += loss(x,y,par)\n",
    "    res /= float(len(x_train))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIC(par,loss):\n",
    "    f1 = D_ast(par,loss)\n",
    "    G = G_ast(par,loss)\n",
    "    Q = Q_ast(par,loss)\n",
    "    Q_inv = np.linalg.pinv(Q)\n",
    "    temp = np.dot(G,Q_inv)\n",
    "    return f1 + np.trace(temp)/float(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_error(par,loss):\n",
    "    res = 0\n",
    "    for x,y in zip(x_test,y_test):\n",
    "        res += loss(x,y,par)\n",
    "    res /= float(len(x_train))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6012687268124246 3.229359045883024\n"
     ]
    }
   ],
   "source": [
    "print(D_ast(par,loss_func),gene_error(par,loss_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
